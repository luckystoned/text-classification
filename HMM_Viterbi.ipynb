{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "858EOFNBPxDs"
      },
      "outputs": [],
      "source": [
        "# instalacion de dependencias previas\n",
        "!pip install conllu\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHDLyQQcboh"
      },
      "source": [
        "# Loading the previously trained HMM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jwZedC6RVJy"
      },
      "outputs": [],
      "source": [
        "# Loading the probabilities of the HMM model\n",
        "import numpy as np \n",
        "transitionProbdict = np.load('transitionHMM.npy', allow_pickle='TRUE').item()\n",
        "emissionProbdict = np.load('emissionHMM.npy', allow_pickle='TRUE').item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Ezi9gcquUw55",
        "outputId": "0b03240b-6688-456b-ecf0-098e1ff6e1a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ADJ',\n",
              " 'ADP',\n",
              " 'ADV',\n",
              " 'AUX',\n",
              " 'CCONJ',\n",
              " 'DET',\n",
              " 'INTJ',\n",
              " 'NOUN',\n",
              " 'NUM',\n",
              " 'PART',\n",
              " 'PRON',\n",
              " 'PROPN',\n",
              " 'PUNCT',\n",
              " 'SCONJ',\n",
              " 'SYM',\n",
              " 'VERB',\n",
              " '_'}"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Identifying the unique POS tags in the corpus\n",
        "stateSet = set([w.split('|')[1] for w in list(emissionProbdict.keys())])\n",
        "stateSet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "-aVD0jboWKGh",
        "outputId": "00d32831-8964-4ad6-de81-ffe5ba2d3a46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ADJ': 9,\n",
              " 'ADP': 13,\n",
              " 'ADV': 0,\n",
              " 'AUX': 14,\n",
              " 'CCONJ': 10,\n",
              " 'DET': 12,\n",
              " 'INTJ': 16,\n",
              " 'NOUN': 7,\n",
              " 'NUM': 11,\n",
              " 'PART': 3,\n",
              " 'PRON': 4,\n",
              " 'PROPN': 8,\n",
              " 'PUNCT': 1,\n",
              " 'SCONJ': 2,\n",
              " 'SYM': 5,\n",
              " 'VERB': 15,\n",
              " '_': 6}"
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Enum categorys with numbers to assign to the columns of the Viterbi matrix\n",
        "tagStateDict = {}\n",
        "for i, state in enumerate(stateSet):\n",
        "  tagStateDict[state] = i\n",
        "tagStateDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SNuWx-ScSTg"
      },
      "source": [
        "# Distribucion inicial de estados latentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "put9Dyk1Yl2A",
        "outputId": "102c4102-3a17-48dd-93b2-40c95a96ff04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ADJ': 0.010882708585247884,\n",
              " 'ADP': 0.16384522370012092,\n",
              " 'ADV': 0.06287787182587666,\n",
              " 'AUX': 0.022370012091898428,\n",
              " 'CCONJ': 0.03325272067714631,\n",
              " 'DET': 0.3633615477629988,\n",
              " 'INTJ': 0.0006045949214026602,\n",
              " 'NOUN': 0.02720677146311971,\n",
              " 'NUM': 0.01995163240628779,\n",
              " 'PART': 0.0018137847642079807,\n",
              " 'PRON': 0.034461910519951636,\n",
              " 'PROPN': 0.1124546553808948,\n",
              " 'PUNCT': 0.07799274486094317,\n",
              " 'SCONJ': 0.02418379685610641,\n",
              " 'SYM': 0.0006045949214026602,\n",
              " 'VERB': 0.04353083434099154,\n",
              " '_': 0.0006045949214026602}"
            ]
          },
          "execution_count": 30,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculating the initial state distribution\n",
        "initTagStateProb = {} # \\rho_i^{(0)}\n",
        "from conllu import parse_incr\n",
        "wordList = []\n",
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
        "count = 0 # Number of sentences in the corpus\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  count += 1\n",
        "  tag = tokenlist[0]['upos']\n",
        "  if tag in initTagStateProb.keys():\n",
        "    initTagStateProb[tag] += 1\n",
        "  else:\n",
        "    initTagStateProb[tag] = 1\n",
        "\n",
        "for key in initTagStateProb.keys():\n",
        "  initTagStateProb[key] /= count\n",
        "\n",
        "initTagStateProb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "J5Rltqj6bbcV",
        "outputId": "2477fc48-e9d1-4548-baf2-e6e218f36b99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 31,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verfying that the sum of the probabilities is 1 (100%)\n",
        "np.array([initTagStateProb[k] for k in initTagStateProb.keys()]).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjnAshwzxrKZ"
      },
      "source": [
        "# Viterbi algorithm construction\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zX-_MnPexnm0"
      },
      "source": [
        "Given a sequence of words $\\{p_1, p_2, \\dots, p_n \\}$, and a set of grammatical categories given by the `upos` convention, the Viterbi probability matrix is considered as follows:\n",
        "\n",
        "$$\n",
        "\\begin{array}{c c}\n",
        "\\begin{array}{c c c c}\n",
        "\\text{ADJ} \\\\\n",
        "\\text{ADV}\\\\\n",
        "\\text{PRON} \\\\\n",
        "\\vdots \\\\\n",
        "{}\n",
        "\\end{array}\n",
        "&\n",
        "\\left[\n",
        "\\begin{array}{c c c c}\n",
        "\\nu_1(\\text{ADJ}) & \\nu_2(\\text{ADJ}) & \\dots  & \\nu_n(\\text{ADJ})\\\\\n",
        "\\nu_1(\\text{ADV}) & \\nu_2(\\text{ADV}) & \\dots  & \\nu_n(\\text{ADV})\\\\\n",
        "\\nu_1(\\text{PRON}) & \\nu_2(\\text{PRON}) & \\dots  & \\nu_n(\\text{PRON})\\\\\n",
        "\\vdots & \\vdots & \\dots & \\vdots \\\\ \\hdashline\n",
        "p_1 & p_2 & \\dots & p_n\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Where the probabilities of the first column (for a category $i$) are given by:\n",
        "\n",
        "$$\n",
        "\\nu_1(i) = \\underbrace{\\rho_i^{(0)}}_{\\text{initial probability}} \\times \\underbrace{P(p_1 \\vert i)}_{\\text{emission}}\n",
        "$$\n",
        "\n",
        "then, for the second column (given a category $j$) they will be:\n",
        "\n",
        "$$\n",
        "\\nu_2(j) = \\max_i \\{ \\nu_1(i) \\times \\underbrace{P(j \\vert i)}_{\\text{transition}} \\times \\underbrace{P(p_2 \\vert j)}_{\\text{emission}} \\}\n",
        "$$\n",
        "\n",
        "thus, in general, the probabilities for column $t$ are given by:\n",
        "\n",
        "$$\n",
        "\\nu_{t}(j) = \\max_i \\{ \\overbrace{\\nu_{t-1}(i)}^{\\text{previous state}} \\times \\underbrace{P(j \\vert i)}_{\\text{transition}} \\times \\underbrace{P(p_t \\vert j)}_{\\text{emission}} \\}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "rAyO788xPKra",
        "outputId": "a470923e-d446-4063-97a7-30e3e962cb18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "9sJhQ35m5ASB",
        "outputId": "7cc43024-fe2c-4e43-a4d1-95952c3195de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 1.47448483e-04, 2.91245828e-10, 0.00000000e+00],\n",
              "       [0.00000000e+00, 2.00411724e-05, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.01922433e-10],\n",
              "       [0.00000000e+00, 0.00000000e+00, 5.02871314e-09, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [8.76142797e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.97926792e-07, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ViterbiMatrix(secuencia, transitionProbdict=transitionProbdict, emissionProbdict=emissionProbdict,\n",
        "            tagStateDict=tagStateDict, initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia)\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # 17 POS tags in the corpus\n",
        "\n",
        "  # First column initialization\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key]\n",
        "    word_tag = seq[0].lower()+'|'+key\n",
        "    if word_tag in emissionProbdict.keys():\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag]\n",
        "\n",
        "  # columns computation\n",
        "  for col in range(1, len(seq)):\n",
        "    for key in tagStateDict.keys():\n",
        "      tag_row = tagStateDict[key]\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys():\n",
        "        # miramos estados de la col anterior\n",
        "        possible_probs = []\n",
        "        for key2 in tagStateDict.keys():\n",
        "          tag_row2 = tagStateDict[key2]\n",
        "          tag_prevtag = key+'|'+key2\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0:\n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        viterbiProb[tag_row, col] = max(possible_probs)\n",
        "\n",
        "  return viterbiProb\n",
        "\n",
        "matrix = ViterbiMatrix('el mundo es pequeño')\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "J9CYMtNpuoKq",
        "outputId": "92aa7ea5-4aec-424f-efd9-1837d02bdf00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('el', 'DET'),\n",
              " ('mundo', 'NOUN'),\n",
              " ('es', 'AUX'),\n",
              " ('muy', 'ADV'),\n",
              " ('pequeño', 'ADJ')]"
            ]
          },
          "execution_count": 48,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ViterbiTags(secuencia, transitionProbdict=transitionProbdict, emissionProbdict=emissionProbdict,\n",
        "            tagStateDict=tagStateDict, initTagStateProb=initTagStateProb):\n",
        "  seq = word_tokenize(secuencia)\n",
        "  viterbiProb = np.zeros((17, len(seq)))  # 17 POS tags in the corpus\n",
        "\n",
        "  # First column initialization\n",
        "  for key in tagStateDict.keys():\n",
        "    tag_row = tagStateDict[key]\n",
        "    word_tag = seq[0].lower()+'|'+key\n",
        "    if word_tag in emissionProbdict.keys():\n",
        "      viterbiProb[tag_row, 0] = initTagStateProb[key]*emissionProbdict[word_tag]\n",
        "\n",
        "  # Next columns computation\n",
        "  for col in range(1, len(seq)):\n",
        "    for key in tagStateDict.keys():\n",
        "      tag_row = tagStateDict[key]\n",
        "      word_tag = seq[col].lower()+'|'+key\n",
        "      if word_tag in emissionProbdict.keys():\n",
        "        # Look at the states of the previous column\n",
        "        possible_probs = []\n",
        "        for key2 in tagStateDict.keys():\n",
        "          tag_row2 = tagStateDict[key2]\n",
        "          tag_prevtag = key+'|'+key2\n",
        "          if tag_prevtag in transitionProbdict.keys():\n",
        "            if viterbiProb[tag_row2, col-1]>0:\n",
        "              possible_probs.append(\n",
        "                  viterbiProb[tag_row2, col-1]*transitionProbdict[tag_prevtag]*emissionProbdict[word_tag])\n",
        "        viterbiProb[tag_row, col] = max(possible_probs)\n",
        "\n",
        "    # Tagging the sequence\n",
        "    res = []\n",
        "    for i, p in enumerate(seq):\n",
        "      for tag in tagStateDict.keys():\n",
        "        if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n",
        "          res.append((p, tag))\n",
        "\n",
        "  return res\n",
        "\n",
        "ViterbiTags('el mundo es muy pequeño')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "FwOk8ABlx13k",
        "outputId": "0dddea10-ed1c-4bc8-f4c7-7cfc16872f7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('estos', 'DET'),\n",
              " ('instrumentos', 'NOUN'),\n",
              " ('han', 'AUX'),\n",
              " ('de', 'ADP'),\n",
              " ('rasgar', 'VERB')]"
            ]
          },
          "execution_count": 66,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ViterbiTags('estos instrumentos han de rasgar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnLT12Qx5D78"
      },
      "source": [
        "# Direct training of HMM with NLTK\n",
        "\n",
        "* Python Class (NLTK) from HMM: https://www.nltk.org/_modules/nltk/tag/hmm.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "NVyCuawh5Eqj",
        "outputId": "8f2ef078-4a92-44cd-f614-543ccbe3cff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title English Treebank\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank\n",
        "train_data = treebank.tagged_sents()[:3900]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "C_DomEIM5Hif",
        "outputId": "df2a8bd9-2e56-4296-ef9a-c774456ae39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Data Structure of the Training Data\n",
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "WtknnYIi5KdG",
        "outputId": "e4dbaf5e-7934-4d24-ab4e-e406620c648d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<HiddenMarkovModelTagger 46 states and 12385 output symbols>"
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Pre-built HMM in NLTK\n",
        "from nltk.tag import hmm\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data)\n",
        "tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "tLG-QzKc5OM4",
        "outputId": "c05f89f9-1894-4116-93b2-f2b7d4cf96a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Pierre', 'NNP'),\n",
              " ('Vinken', 'NNP'),\n",
              " ('will', 'MD'),\n",
              " ('get', 'VB'),\n",
              " ('old', 'JJ')]"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagger.tag(\"Pierre Vinken will get old\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "aGLYRUBb5Wni",
        "outputId": "2ed13ca6-f699-4602-a94f-ef2753f8e461"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9815403947224078"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title training accuracy\n",
        "tagger.evaluate(treebank.tagged_sents()[:3900])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN-Bgfk-pI0m"
      },
      "source": [
        "## Practice Exercise\n",
        "\n",
        "**Objective:** Train an HMM using the `hmm.HiddenMarkovModelTrainer()` class on the `UD_Spanish_AnCora` dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrtTL4ihxfiq"
      },
      "source": [
        "1. **Pre-processing:** In the previous example, we used the English `treebank` dataset, which has a different structure compared to `AnCora`. In this part, write code to transform the structure of `AnCora` so that it matches the structure of the `treebank` dataset as follows:\n",
        "\n",
        "$$\\left[ \\left[ (\\text{'El'}, \\text{'DET'}), (\\dots), \\dots\\right], \\left[\\dots \\right] \\right]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1X8qg5Fc5ahS"
      },
      "outputs": [],
      "source": [
        "# Solving practice exercises\n",
        "!pip install conllu\n",
        "!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git\n",
        "from conllu import parse_incr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "data_array = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  tokenized_text = []\n",
        "  for token in tokenlist:\n",
        "    tokenized_text.append((token['form'], token['upos']))\n",
        "  data_array.append(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(data_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_OYeCVQrZAK"
      },
      "source": [
        "2. **Training:** Once the dataset is in the correct structure, use the `hmm.HiddenMarkovModelTrainer()` class to train with 80% of the dataset as the `training` set and 20% as the `test` set.\n",
        "\n",
        "**Hint:** For the separation between training and test sets, you can use the function from Scikit Learn:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "At this point, the Machine Learning with Scikit Learn course is a good complement to better understand the functionalities of Scikit Learn: https://platzi.com/cursos/scikitlearn-ml/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZpAIB87sTqL"
      },
      "outputs": [],
      "source": [
        "# desarrolla tu código aquífrom sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data_array, test_size=0.2, random_state=42)\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tag import hmm\n",
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data)\n",
        "tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLS54wqsu9OK"
      },
      "source": [
        "3. **Validación del modelo:** Un vez entrenado el `tagger`, calcula el rendimiento del modelo (usando `tagger.evaluate()`) para los conjuntos de `entrenamiento` y `test`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEwZIG8Du98v"
      },
      "outputs": [],
      "source": [
        "tagger.evaluate(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tagger.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Observations\n",
        "\n",
        "* If you use the `es_ancora-ud-dev.conllu` dataset, you will notice that it is very small. You can try with `es_ancora-ud-train.conllu`.\n",
        "\n",
        "* In practice, it is customary to train the model with `es_ancora-ud-train.conllu` and validate the test with `es_ancora-ud-test.conllu`. The `es_ancora-ud-dev.conllu` file is typically used for quick prototyping of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tagger = hmm.HiddenMarkovModelTrainer().train_supervised(data_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_file = open(\"UD_Spanish-AnCora/es_ancora-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
        "test_array = []\n",
        "for tokenlist in parse_incr(data_file):\n",
        "  tokenized_text = []\n",
        "  for token in tokenlist:\n",
        "    tokenized_text.append((token['form'], token['upos']))\n",
        "  test_array.append(tokenized_text)\n",
        "len(test_array)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cAHDLyQQcboh",
        "9SNuWx-ScSTg",
        "YjnAshwzxrKZ",
        "BnLT12Qx5D78"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
