{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu9C9XvfOl_J"
      },
      "source": [
        "# NLTK Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AlvswJiYTdj"
      },
      "source": [
        "## Eng basic Pipeline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Tmb2Cr7v92E7",
        "outputId": "dce89768-7703-4de9-faef-851b9fe7f700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting click (from nltk)\n",
            "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (1.3.2)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/23/d3/65db07106b4584b62b1ec0061277c54f565da92d91f0a211057faccb841b/regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata\n",
            "  Downloading regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk)\n",
            "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/18/eb/fdb7eb9e48b7b02554e1664afd3bd3f117f6b6d6c5881438a0b055554f9b/tqdm-4.66.4-py3-none-any.whl.metadata\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl (278 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.5/278.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
            "Successfully installed click-8.1.7 nltk-3.8.1 regex-2024.5.15 tqdm-4.66.4\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
            "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
            "[nltk_data]     [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify\n",
            "[nltk_data]     failed: unable to get local issuer certificate\n",
            "[nltk_data]     (_ssl.c:1000)>\n"
          ]
        }
      ],
      "source": [
        "#@title prev deps\n",
        "%pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "8lefQX__OtfL",
        "outputId": "366cab1c-6f49-40cc-da9c-2c1efbe2baf4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'text' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mpos_tag(\u001b[43mtext\u001b[49m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ],
      "source": [
        "#@title One line tagger\n",
        "text = word_tokenize(\"And now here I am enjoying today\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "qocvMilXPdXs",
        "outputId": "0c14c16e-259a-485d-d440-bc99e727d66e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "None\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "None\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "None\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "None\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "None\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "#@title Gramatical Category of each tag\n",
        "nltk.download('tagsets')\n",
        "for tag in ['CC', 'RB', 'PRP', 'VBP', 'VBG', 'NN']:\n",
        "  print(nltk.help.upenn_tagset(tag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "MEo2MWrkPqrO",
        "outputId": "3a57539a-8a0f-4476-e722-c65014e43926"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('They', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " ('not', 'RB'),\n",
              " ('permit', 'VB'),\n",
              " ('other', 'JJ'),\n",
              " ('people', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('residence', 'NN'),\n",
              " ('permit', 'NN')]"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Homonyms words\n",
        "text = word_tokenize(\"They do not permit other people to get residence permit\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIIi46WzYZvc"
      },
      "source": [
        "## Etiquetado en Español\n",
        "\n",
        "Para el ingles, NLTK tiene tokenizador y etiquetador pre-entrenados por defecto. En cambio, para otros idiomas es preciso entrenarlo previamente.\n",
        "\n",
        "* usamos el corpus `cess_esp` https://mailman.uib.no/public/corpora/2007-October/005448.html\n",
        "\n",
        "* el cual usa una convención de etiquetas gramaticales dada por el grupo EAGLES https://www.cs.upc.edu/~nlp/tools/parole-sp.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "1uLX4KgDX-cs",
        "outputId": "37b51208-4c6c-4ca7-a969-c67a9ac040c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('cess_esp')\n",
        "from nltk.corpus import cess_esp as cess\n",
        "from nltk import UnigramTagger as ut\n",
        "from nltk import BigramTagger as bt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "mFi7BLojYw-F",
        "outputId": "35b1c62f-46fe-4f5d-f8b9-2ee0525e7083"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8069484240687679"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Entrenamiendo del tagger por unigramas\n",
        "fraction = int(len(cess_sents)*90/100)\n",
        "cess_sents = cess.tagged_sents()\n",
        "uni_tagger = ut(cess_sents[:fraction])\n",
        "uni_tagger.evaluate(cess_sents[fraction+1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "HHA32_xVg81K",
        "outputId": "55a64d99-9465-4337-f6ee-42bafd25cb84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Yo', 'pp1csn00'),\n",
              " ('soy', 'vsip1s0'),\n",
              " ('una', 'di0fs0'),\n",
              " ('persona', 'ncfs000'),\n",
              " ('muy', 'rg'),\n",
              " ('amable', 'aq0cs0')]"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uni_tagger.tag(\"Yo soy una persona muy amable\".split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "id": "kDIt1k1ubB9D",
        "outputId": "cffe9d86-fd47-4c4a-d57f-69702843f307"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1095272206303725"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Entrenamiento del tagger por bigramas\n",
        "fraction = int(len(cess_sents)*90/100)\n",
        "bi_tagger = bt(cess_sents[:fraction])\n",
        "bi_tagger.evaluate(cess_sents[fraction+1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "nLCTRHeHfiY3",
        "outputId": "bfc54ac9-6a05-4a2b-d541-72382d29bd94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Yo', 'pp1csn00'),\n",
              " ('soy', 'vsip1s0'),\n",
              " ('una', None),\n",
              " ('persona', None),\n",
              " ('muy', None),\n",
              " ('amable', None)]"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bi_tagger.tag(\"Yo soy una persona muy amable\".split(\" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m50bRKLlikbx"
      },
      "source": [
        "# Enhanced Tagging with Stanza (StanfordNLP)\n",
        "\n",
        "**What is Stanza?**\n",
        "\n",
        "* The Stanford NLP research group had a suite of libraries that performed various NLP tasks. This suite was unified into a single service called **CoreNLP**, based on Java code: [CoreNLP](https://stanfordnlp.github.io/CoreNLP/index.html)\n",
        "\n",
        "* For Python, there is **StanfordNLP**: [StanfordNLP](https://stanfordnlp.github.io/stanfordnlp/index.html)\n",
        "\n",
        "* However, **StanfordNLP** has been deprecated, and the new versions of the NLP suite are maintained under the name **Stanza**: [Stanza](https://stanfordnlp.github.io/stanza/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "WY_YTM6TirMd",
        "outputId": "241e0bb1-e160-436a-904f-7bedc7ae01e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/9c/60689521a971a57dd02d2925105efedefa9dccd76c9a0b92566683d43e89/stanza-1.0.1-py3-none-any.whl (193kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 14.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.5.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (47.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "HG-Sdn6QmHgR",
        "outputId": "dcd469a4-37f0-4230-f24b-4335aea9a118"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 115kB [00:00, 10.1MB/s]                    \n",
            "2020-07-13 01:50:46 INFO: Downloading default packages for language: es (Spanish)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.0.0/es/default.zip: 100%|██████████| 583M/583M [01:48<00:00, 5.36MB/s]\n",
            "2020-07-13 01:52:48 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "stanza.download('es')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "B2sKNXawmWH7",
        "outputId": "b1cfbeea-b82c-4100-fa0c-97a9203ffbc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2020-07-13 01:53:05 INFO: Loading these models for language: es (Spanish):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | ancora  |\n",
            "| pos       | ancora  |\n",
            "=======================\n",
            "\n",
            "2020-07-13 01:53:06 INFO: Use device: cpu\n",
            "2020-07-13 01:53:06 INFO: Loading: tokenize\n",
            "2020-07-13 01:53:06 INFO: Loading: pos\n",
            "2020-07-13 01:53:07 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "nlp = stanza.Pipeline('es', processors='tokenize,pos')\n",
        "doc = nlp('yo soy una persona muy amable')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "86rL3IJRm1oI",
        "outputId": "d6a637a1-1f83-4cbc-b854-1be92012ddfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yo PRON\n",
            "soy AUX\n",
            "una DET\n",
            "persona NOUN\n",
            "muy ADV\n",
            "amable ADJ\n"
          ]
        }
      ],
      "source": [
        "for sentence in doc.sentences:\n",
        "  for word in sentence.words:\n",
        "    print(word.text, word.pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsZF93NQodxX"
      },
      "source": [
        "# Additional References:\n",
        "\n",
        "* POS Tagging with Stanza: [Accessing POS and Morphological Feature for Word](https://stanfordnlp.github.io/stanza/pos.html#accessing-pos-and-morphological-feature-for-word)\n",
        "\n",
        "* Stanza | Github: [Stanza GitHub Repository](https://github.com/stanfordnlp/stanza)\n",
        "\n",
        "* Article on ArXiv: [PDF](https://arxiv.org/pdf/2003.07082.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2gkMtDvyK_F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
